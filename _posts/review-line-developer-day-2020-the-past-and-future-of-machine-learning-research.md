---
title: "Review: LINE Developer Day 2020 - The past and future of machine learning research"
slug: review-line-developer-day-2020-the-past-and-future-of-machine-learning-research
date: '2021-02-04T03:47:49.000Z'
tags: AI, Computer Science
coverImage: /assets/blog/review-line-developer-day-2020-the-past-and-future-of-machine-learning-research/cover.jpeg
author:
  name: Pittawat Taveekitworachai
  picture: /assets/blog/authors/pittawat.jpg
ogImage:
  url: /assets/blog/review-line-developer-day-2020-the-past-and-future-of-machine-learning-research/cover.jpeg
excerpt: ใน Session นี้ คุณ​ Masashi Sugiyama ได้มาพูดถึงใน 3 ประเด็นหลัก ๆ ด้วยกัน นั่นคือ Trend in ML Research, Our Research, Future ML Research
---

ใน Session นี้ คุณ​ Masashi Sugiyama, Director at RIKEN AIP ได้มาพูดถึงใน 3 ประเด็นหลัก ๆ ด้วยกัน นั่นคือ

1. Trend in ML Research
2. Our Research
3. Future ML Research

---

## Trend in ML Research

![Allie and Jacob attended KinnektorCon to create new connections within the Fox Valley and spread the word about Startup Wisconsin Week and how the community can get involved.](https://images.unsplash.com/photo-1540575467063-178a50c2df87?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDR8fGNvbmZlcmVuY2V8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

โดยในหัวข้อของ Trend in ML Research นั้น คุณ​ Sugiyama ได้เลือกที่จะพูดถึง machine learning conferences สอง conferences ได้แก่ ICML (International Conference on Machine Learning) และ NeuralPS (Neural Information Processing Systems) โดยสิ่งที่แตกต่างกันระหว่างสองเจ้านี้ก็คือ ICML นั้น จะเน้นไปในเรื่องของ machine learning ทั่ว ๆ ไป (เรียนรู้จากข้อมูล)​ ในขณะที่ NeuralPS นั้นจะโฟกัสมากขึ้นไปยัง neuro-inspired AI ซึ่งเกี่ยวข้องกับ Neural/Cognitive science

โดยทั้งสอง conferences นั้นในช่วงปีที่ผ่านมาหากเทียบสถิติจำนวนผู้เข้าร่วม, จำนวน Paper ที่ส่งเข้ามา,​ จำนวน Paper ที่ได้รับการอนุมัติ จะพบว่าเพิ่มขึ้นอย่างมากหากเทียบระหว่างปี 2013 และ 2019 นอกจากนี้บริษัททั่วไปโลกยังให้การสนับสนุนเพิ่มมากขึ้นอีกด้วย

![Play with UV light.](https://images.unsplash.com/photo-1495592822108-9e6261896da8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDN8fG1hY2hpbmUlMjBsZWFybmluZ3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

โดยสิ่งที่น่าสนใจก็คือเมื่อเทียบระหว่างปี 2015 และ 2019 แล้วจะพบว่า ในปี 2015 นั้นโฟกัสจะอยู่งานวิจัยเกี่ยวกับ machine learning โดยตรง เช่น AlphaGo, รถยนต์ขับเคลื่อนอัตโนมัติ ก่อให้เกิดความคาดหวังที่สูงในเทคโนโลยี machine learning และบริษัทส่วนใหญ่ที่สนใจยังเป็นบริษัทจากสหรัฐอเมริกา 

แต่ทว่าในปี 2019 โฟกัสได้ย้ายไปอยู่กับผลกระทบเชิงสังคมมาหยิ่งขึ้น เช่น เรื่องของความเป็นส่วนตัว ความเท่าเทียมกัน ความรับผิดชอบ และการตอบสนองต่อความต้องการของสังคม กล่าวได้ว่าจะทำยังไงให้ AI หรือ ML สามารถที่จะช่วยสนับสนุนให้งานวิจัยเชิงวิทยาศาสตร์ในสาขาต่าง ๆ เช่น เคมี หรือชีววิทยา เป็นไปได้รวดเร็วยิ่งขึ้น 

นอกจากนี้ยังมีอีกเทรนที่น่าสนใจคือบริษัทจากจีนเรื่องเข้ามาแข่งขันกับบริษัทจากสหรัฐฯ แล้วในตอนนี้ ทำให้สังคมของนักวิจัยเองเติบโตอย่างก้าวกระโดดแต่ก็เฉพาะในประเทศที่พัฒนาแล้ว ทำให้ในช่วงหลัง conferences เองก็ช่วยสนับสนุนกลุ่ม minority มากยิ่งขึ้น เช่น กลุ่มผู้หญิง, กลุ่มคนผิวสี

---

## Our Research

สำหรับในหัวข้อนี้คุณ Sugiyama ก็ได้มาแชร์เกี่ยวกับงานวิจัยที่ตัวเองกำลังทำอยู่ ซึ่งตัวคุณ Sugiyama เองได้สนใจในสองเรื่อง นั่นก็คือเรื่องของ Robustness และ Weakly supervised data

![](https://images.unsplash.com/photo-1559734597-deb46a80b958?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDh8fHNvY2lldHl8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

สำหรับ Robustness นั้น เราต้องเข้าใจก่อนว่าโดยพื้นฐานแล้ว ML นั้นจะเก่งขึ้นได้ก็ด้วย data ซึ่งเอาไปใช้ในการ train แต่ว่า data ที่เก็บมานั้นเราหลีกเลี่ยงไม่ได้ที่จะมีสิ่งที่เรียกว่า noise ติดมาด้วย โดยเฉพาะข้อมูลจากพวกเซ็นเซอร์ IoT หรือข้อมูลที่ label โดยมนุษย์ ซึ่งอาจจะมี human error ติดมาด้วย ดังนั้นเราจะมีวิธีไหนบ้างที่จะช่วยให้ยังมั่นใจได้ว่า data ของเรายังคง Robust มากพอที่จะนำไป train model ของเรา

นอกจากนี้ยังมีเรื่องของ Bias ที่มักจะพบใน data ได้ เนื่องจากในปัจจุบันเราต้องใส่ใจกับเรื่องของความเป็นส่วนตัวมากยิ่งขึ้น นั่ทำให้เราเก็บข้อมูลได้ยากขึ้น ทำให้ข้อมูลสำหรับใช้ในการ test หรือ train มีจำกัด และข้อมูลก็มีโอกาสสูงมากที่จะแตกต่างจากข้อมูลที่อาจจะได้พบในการใช้จริง จึงเป็นสิ่งที่น่าสนใจว่าจะมีวิธีไหนบ้างที่จะมาจัดการกับเรื่องนี้

โดยคุณ Sugiyama ได้นำเสนอวิธีไว้ทั้งหมด 3 วิธี

## 1. Noisy Label Learning

![Lost Places](https://images.unsplash.com/photo-1517061388989-aec7379dfce0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEzfHxkaXJ0eXxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

การทำ classification แบบทั่วไปนั้นมักจะต้องการชุดข้อมูลที่สะอาด (clean training data) ซึ่งจะทำให้เราได้ชุดข้อมูลในรูปแบบ \({(x_i, y_i)}\) โดย x เป็นข้อมูลนำเข้า (input pattern) ส่วน y คือ label โดยเราสามารถคำนวณ training error ได้จากสูตร \( \frac{1}{n}\sum_{i=1}^{n}l(y_i, g(x_i)) \) โดย \(g(x) \in I\!R^C\) คือ classifier ส่วน \(l(y, g(x)) \in I\!R\) คือ loss โดยจุดมุ่งหมายคือเราพยายามลด loss ให้ได้เพื่อไปให้ถึง optimum solution

อย่างไรก็ตามสำหรับการ classifition ด้วยชุดข้อมูลที่ noisy นั้น เราจะได้ชุดข้อมูลในรูปแบบ \({(x_i, \tilde{y_i})}\) โดย \(\tilde{y_i}\) จะแสดงถึง noisy label ดังนั้นการพยายามที่จะ optimize ให้ได้ optimum solution ผ่านการลด training error จะไม่ได้ผลอีกต่อไป

อย่างไรก็ตามเราก็ยังมีบางวิธีอยู่สำหรับการจัดการปัญหานี้

วิธีการพื้นฐาน (standard approaches)

1. Unsupervised outleir removal คือการพยายามระบุว่า data point ไหนเป็น outlier และลบมันออกไป อย่างไรก็ตามการใช้วิธีนี้นั้นยากกว่าตัวปัญหาหลัก จึงทำงานได้ไม่ค่อยดีนัก
2. Robust loss, Regularization สำหรับวิธีนี้คือการที่เรานำ parameters ขนาดไม่ใหญ่มากนักมาพยายาม regularization ซึ่งทำงานได้ดีกับชุดข้อมูลที่มี noise ไม่มากนัก หากมีจำนวนมากเราอาจไม่สามารถการันตี robustness ได้
3. New appraches วิธีการใหม่ ซึ่งคุณ Sugiyama เสนอไว้ด้วยกันทั้งสิ้น 3 แบบ

![Hacker binary attack code. Made with Canon 5d Mark III and analog vintage lens, Leica APO Macro Elmarit-R 2.8 100mm (Year: 1993)](https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fG1hdHJpeHxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

3.1 Noise transition matrix \(T\) โดย \(T\) เป็น square matrix ขนาดเท่ากับจำนวน class label represents probability ของการ flipping (เช่น การเปลี่ยนจาก \(\tilde{y}\)) เป็น \(y\)) โดยเรามีด้วยกัน 2 วิธีย่อย

3.1.1 Loss correction สำหรับวิธีนี้เราจะใช้ \(T^{-1}\) ในการกำจัด noise ออกไป

3.1.2 Classifier correction สำหรับวิธีนี้เราจะใช้ \(T^{T}\) ในการจำลอง noise หรือพูดในอีกแง่หนึ่งคือการที่เราพยายามจำลอง noise ให้เกิดขึ้นใน output เช่นกัน (เท่านี้ทั้งสองฝั่งก็จะเท่าเทียมกัน)

ปัญหาสำคัญในวิธีนี้อยู่ที่ว่าเราจะ esitmate \(T\) ได้ยังไง ซึ่งเราอาจทำได้โดยการใช้ Ancjor point (ข้อมูลจำนวนน้อย ๆ ที่ไม่มี noise) มาประมาณ \(T\) อย่างไรก็ตามข้อมูลที่ clean ก็ยังคงหายากอยู่ดี ดังนั้นวิธีการสำหรับการประมาณ \(T\) ยังคงอยู่ในขั้นตอนของการวิจัย

![teach me senpai](https://images.unsplash.com/photo-1522881193457-37ae97c905bf?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDV8fHRlYWNoaW5nfGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

3.2 Co-teaching วิธีการนี้จะค่อนข้างจำเพาะสำหรับ neural net ที่มีการใช้ stochastic gradient descent โดยวิธีการนี้จะมองว่าในระหว่างการเรียนรู้นั้น model จะได้เจอทั้ง data ที่  clean และ data ที่มี noise โดยสิ่งที่แตกต่างกันคือ data ที่ clean จะสามารถ fit ได้อย่างง่ายดายและรวดเร็ว ในขณะที่ data ที่มี noise จะ fit ได้ยาก ซึ่งเราสามารถที่จะนำ early stopping เข้ามาช่วยได้โดยการ fit เฉพาะ data ที่ clean แล้วหยุด อย่างไรก็ตามก็มีข้อควรระวังสำหรับการทำ early stopping ที่ต้องตัดสินใจอย่างรอบคอบ

สำหรับวิธีการแบบ Co-teaching นั้นจะสร้าง neural net ที่เหมือนกันทุกประการขึ้นมาสองอัน โดยแต่ละ net จะเรียนรู้อย่างไม่เกี่ยวข้องกับอีก net หนึ่ง และเราจะแบ่ง data ออกเป็นหลาย ๆ mini batch โดย neural net จะสุ่มเลือก mini batch ขึ้นมา และเรียนรู้เฉพาะส่วนที่เป็น clean data (ซึ่งมีน้อย แต่ไม่มี noise 😜)  จากนั้นจะส่ง data ส่วนที่ clean ไปให้อีก net หนึ่งทำการเรียนรู้ ด้วยวิธีการนี้เราสามารถลด noise ได้พร้อมกับได้ robustness นั่นเอง

อย่างไรก็ตามข้อควรระวังคือ net ทั้งสองจะ produce output ที่แตกต่างกันซึ่งเราต้องจัดการตรงส่วนนั้นต่อ นอกจากนี้จริง ๆ แล้วข้อมูลที่มี noise ก็ไม่ได้แย่ไปหมดซะทีเดียว เพราะมันยังมีข้อมูลบางอย่างอยู่เช่นกัน อีกหนึ่งสิ่งคือในระหว่างการ train หากเป็นตามปกติอาจจะเกิด overfit จากการเผลอเลือก data ที่มี noisy แล้ว ascend ลงไปแต่ทางนั้นจนเจอ local minimum ซึ่งเราอาจจะแก้ไขได้ด้วยการสุ่มเลือก mini batch เพื่อไปทางอื่น ๆ บ้างนั่นเอง

![](https://images.unsplash.com/photo-1488278905738-514111aa236c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDE3fHxvY2VhbnxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

3.3 Flooding หลังจากที่เรา Train model ผ่านไปสักระยะในระหว่างการ train อาจจะเริ่มปรากฎสัญญาณของ overfitting เช่น การที่ traning loss กำลังมุ่งหน้าเข้าใกล้ 0 แต่ testing loss ที่กำลังลงยู่เหมือนกันกลับเริ่มเพิ่มขึ้นเรื่อย ๆ จนปรากฎเป็นช่องว่างขนาดใหญ่ แน่นอนว่าเรามักจะแก้ปัญหานี้กันด้วยการทำ early stopping

ทางคุณ Sugiyama ได้เสนออีกแนวคิดหนึ่งขึ้นมา คือ แทนที่เราจะปล่อยให้ training loss มุ่งหน้าลงใต้จนเกือบแตะระดับ 0 นั้น เราจะมีสิ่งที่เรียกว่า Flooding level ขึ้นมา โดย training loss จะไม่ลงไปที่ระดับต่ำกว่านี้ ทำให้ testing loss กลับมาลดลงอีกครั้ง จากนั้นจึงนำ flooding ออก หาก training loss เริ่มมีช่องว่างขนาดใหญ่กับ testing loss อีกครั้งเราก็สามารถนำ flooding level มากันไว้ได้อีกครั้ง เพื่อให้ testing loss กลับมาลดลงอีกครั้ง โดยวิธีการนี้คุณ Sugiyama เรียกว่า (Epochwise) Double descent ครับ

## 2. Weakly Supervised Learning

![](https://images.unsplash.com/photo-1559843788-693858bf7338?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEwfHxjb25zdHJ1Y3R8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

สำหรับ supervised learning แบบทั่วไปนั้น มักจะต้องการ Fully input output pair แต่ในหลายกรณี labeled data มักจะหาได้ยาก หรือมีค่าใช้จ่ายที่สูงในการทำ เช่น ข้อมูลทางการแพทย์ เพื่อให้สามารถนำข้อมูลที่ไม่สมบูรณ์มาใช้ในการ training ได้ จึงมีวิธีด้วยกันทั้งหมด 2 วิธีครับ

2.1 Complementary level สำหรับในวิธีนี้นั้นจะพลิกกลับกับการ predict โดยทั่วไปครับ คือแทนที่จะ predict ว่า vector นี้จะเป็น class ใด เราจะ predict ว่าจะไม่เป็น class ใดแทน โดยเราเรียก label แบบนี้ว่า complementary level ซึ่งจะทำให้สามารถ train ได้ง่ายยิ่งขึ้น และจะมี learning error อยู่ที่ประมาณ \(\frac{1}{\sqrt{n}}\)

2.2 Partial-labeled classification แทนที่เราจะ predict ว่า x นั้นเป็น class A หรือ class B เราจะ predict ว่า x เป็น either class A or B แทน ด้วยวิธีนี้ทำให้ x อาจจะมีได้มากกว่า 1 คำตอบ และทำให้เราสามารถเก็บข้อมูลได้ง่ายยิ่งขึ้นครับ ในวิธีนี้จะมี learning error อยู่ที่ประมาณ \(\frac{1}{\sqrt{n}}\) เช่นเดียวกับวิธีก่อนหน้าครับ

![Amish couple walking railroad tracks](https://images.unsplash.com/photo-1526352917800-210e5273bffe?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDM0fHx0d298ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

นอกจากนี้ยังมีกรณีพิเศษนั่นก็คือ binary classification ที่เราจะมีวิธีการอื่น ๆ อีกมากมายด้วยครับ

- Positive-Unlabeled ใช้ข้อมูลที่ label แค่ positive class เพียงอย่างเดียว
- Positive confidence ระบุระดับความมั่นใจว่าจะเป็น positive กี่เปอร์เซ็นต์
- Unlabeled-Unlabeled ไม่มีการ label เลย แต่ใช้การหาความแตกต่างของ features
- Positive-Negative-Unlabeled
- Similar-Disimilar-Unlabeled

## 3. Bias

![](https://images.unsplash.com/photo-1556565681-306458ef93cd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGluY2x1c2lvbnxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

สำหรับหัวข้อนี้เหมือนกับการเตือนถึงข้อระวังมากกว่าครับ คือเราควรระวัง bias ที่อาจเกิดขึ้น เช่น environment ที่แตกต่างกัน อย่างถ้าเราอยากทำระบบตรวจจับใบหน้า ข้อมูลที่เราเก็บมาอาจจะเก็บมาสำหรับ train model อาจจะเป็นรูปถ่ายใบหน้าที่ถ่ายในสตูดิโอ แต่ว่าพอนำไปใช้งานจริงสภาพแสงโลกจริงอาจจะไม่ได้เหมือนแสงในสตูดิโอเสมอไป

สำหรับวิธีการแก้ไขนั้น คุณ​ Sugiyama ได้นำเสนอไว้หนึ่งวิธีนั่นก็คือการใช้ Transfer learning เพื่อช่วยในการ match distribution ของ training และ testing dat ครับ โดยเฉพาะ unsupervised transfer learning

---

## Future ML Research

![White robot human features](https://images.unsplash.com/photo-1485827404703-89b55fcc595e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fHJvYm90fGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

ในปัจจุบันเราเห็นการนำ ML ไปประยุกต์ใช้ในระบบต่าง ๆ มากมาย ไม่ว่าจะเป็น image recognition, specch recognition, language translation ซึ่งในหลาย ๆ กรณีก็ทำได้ดีจนเทียบเท่ามนุษย์ หรือในบางกรณีก็เก่งกาจกว่ามนุษย์ซะอีก นอกจากนี้งานที่มีลักษณะเป็น routine ก็อาจจะโดนแทนที่ได้อย่างง่ายดายด้วย AI

แต่ถึงจะพูดอย่างนี้นั้น AI ก็ยังคงมีข้อจำกัดมากมาย เช่น ในงานที่ต้องใช้ความคิดสร้างสรรค์ งานที่เป็น low-level ก็ยังคงต้องใช้มนุษย์อยู่ ทฤษฎีที่ว่า AI จะมาเป็นภัยคุกคามมนุษย์นั้นก็จะยังคงอยู่ต่อไป แต่เราไม่ควรไปกังวลกับมันมากนัก เพราะยังมีอีกหลายสิ่งที่ AI ยังทำไม่ได้และยังอยู่ในขั้นตอนของงานวิจัย

**Summary of past and present AI research**

ในอดีตเราจะพบ AI อยู่สองประเภท คือ Logical AI และ Neuro-inspired AI

 โดย Logical AI ในทศวรรษที่ 1960 จะมุ่งเน้นไปที่ inference และ search พอมาถึงทศวรรษที่ 1980 ก็เริ่มมี expert systems และ AI แบบ knowledge bases

ในขณะที่ Neuro-inspired AI ในช่วงทศวรรษที่ 1960 นั้นจะเป็น single-layer perceptron ก่อนจะได้รับการพัฒนาเป็น multi-layer perceptron ในช่วงทศวรรษที่ 1990

ตัดภาพมาที่ปัจจุบันซึ่งเป็น AI แบบ statistical machine learning based โดยเริ่มต้นในทศวรรษที่ 2000 ซึ่งจะเป็นพวก frequentist statistics, convex optimization, Bayesian statistics ก่อนที่ในทศวรรษที่ 2010 จะขยับมายัง deep learing

![As Kuromon Market in Osaka was about to close for the evening I sampled some delicious king crab and did a final lap of the market when I stumbled upon one of the most Japanese scenes I could possibly imagine, a little girl, making friends with a robot.](https://images.unsplash.com/photo-1507146153580-69a1fe6d8aa1?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDR8fHJvYm90JTIwaHVtYW58ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

สำหรับในอนาคตนั้น คุณ Sugiyama เชื่อว่าเราจะได้พบกับ Human-like AI ที่จะรวมทุกความรู้ที่ผ่านมาในอดีตเพื่อสร้าง AI ที่ความเหมือนมนุษย์นั้นเอง

แต่ถึงจะพูดอย่างนั้นก็ตาม human-like AI ก็คงไม่ใช่ AI ล้างโลกซะทีเดียว เพราะคุณ Sugiyama เชื่อว่ามันไม่จำเป็นเสมอไปที่ AI จำต้องฉลาดด้วยตัวของมันเอง แต่มันอาจเข้ามาเป็ฯส่วนหนึ่งในสังคมของมนุษย์มากกว่า ด้วยสิ่งที่เรียกว่า Co-intelligence ซึ่งคือการที่มนุษย์และเครื่องจักรเรียนรู้ แลกเปลี่ยนกัน และทำงานร่วมกัน เช่น Fashion Show ที่มหาวิทยาลัยโตเกียวจัดขึ้นในปี 2019 ซึ่งทุกชุดนั้นออกแบบโดย Fashion designer ที่เป็นมนุษย์ร่วมกับ AI ที่เป็น generative adversarial net + neural style transfer

ดังนั้น AI ในอนาคตก็ควรที่จะเป็น AI ที่ Human-inclusive คือ AI ที่ต้องประกอบไปด้วย

1. Math-oriented ML แน่นอนว่าการพัฒนาทางคณิตศาสตร์จะยังคงต้องอยู่ต่อไปเพื่อให้ได้ model ที่มีประสิทธิภาพมากยิ่งขึ้น
2. Human learning เราต้องศึกษาการเรียนรู้ของมนุษย์ให้มากขึ้น เช่นในเชิงประสาทวิทยาเพื่อนับมาปรับใช้กับ AI
3. Human assistance การที่ AI จะเป็นผู้ช่วยมนุษย์ โดยมนุษย์จะใช้ทั้งความรู้และความคิดสร้างสรรค์โดยมี AI เป็นผู้สนับสนุน
4. Human society AI ที่จะถูกสร้างขึ้นมานั้นต้องคำนึงถึงบริบทของสังคมมนุษย์ เช่น วัฒนธรรม หรือจริยธรรมในสังคม

![](https://images.unsplash.com/photo-1578347371317-f67733551451?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDI4fHxyb2JvdCUyMGh1bWFufGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000)

กล่าวได้ว่า AI ที่จะประสบความสำเร็จนั้นต้องเกิดจากวคามเข้าใจกิจกรรมในสมองและความเข้าใจในพฤติกรรมทางสังคมของมนุษย์แบบ high level ซึ่งเป็นสิ่งที่ท้าทายเป็นอย่างมากในการพัฒนาให้เกิดขึ้น สรุปได้ว่า AI นั้นควรที่จะต้องเข้าใจสังคมของมนุษย์และเป็นส่วนหนึ่งของสังคมมนุษย์นั่นเอง

---

สำหรับผู้ที่สนใจสามารถติดตาม Talk ฉบับเต็มได้ที่ [LINE Developer Day](https://linedevday.linecorp.com/2020/en/sessions/6613)

---

## *📚 Hope you enjoy reading! 📚*

---
